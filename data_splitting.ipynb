{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73e641a",
   "metadata": {},
   "source": [
    "## Purpouse of this notebook is to create new datasets from the one we have,\n",
    "### Firstly create separate folders for various depths,  one for averaged, and one for our base dataset\n",
    "### Then perform cleaning based on data_cleaning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '2022.11.07-merged-single-observation'\n",
    "FILENAMES = [f for f in listdir(DATA_PATH) if (isfile(join(DATA_PATH, f)) and not f.startswith('.'))]\n",
    "DEPTHS = [ 100, 500, 1000, 1500, 2000, 2500]\n",
    "MERGED_NAME = 'CLEANED_MERGED_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c365fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function we will split by \n",
    "\n",
    "## helpful filters and statistical functions\n",
    "\n",
    "def fix_date_formatting(df):\n",
    "    df['DATE'] = pd.to_datetime(df.DATE, infer_datetime_format=True)\n",
    "    return df\n",
    "\n",
    "def filter_by_months(df, months):\n",
    "    return df[df['DATE'].dt.month == any(months)]\n",
    "\n",
    "def group_by_year(df, par):\n",
    "    df =  df.groupby(df['DATE'].dt.year)[par].mean()\n",
    "    return pd.DataFrame({\"DATE\":df.index, par:df.values})\n",
    "\n",
    "def group_by_date(df, par):\n",
    "    df =  df.groupby(df['DATE'].dt.date)[par].mean()\n",
    "    return pd.DataFrame({\"DATE\":df.index, par:df.values})\n",
    "\n",
    "def group_by_month(df, par):\n",
    "    #df = df.drop('DEPTH', axis=1)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.groupby(pd.Grouper(freq='M', key='DATE')).mean()\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def filter_by_depth(df, depth):\n",
    "    df = df.loc[df['DEPTH'] == depth]\n",
    "    return df\n",
    "\n",
    "def filter_by_depth_range(df, low, high):\n",
    "    df = df.loc[low <= df['DEPTH'] <= high]\n",
    "    return df\n",
    "\n",
    "def group_by_depth(df, par):\n",
    "    df =  df.groupby(df['DEPTH'])[par].mean()\n",
    "    return pd.DataFrame({\"DEPTH\":df.index, par:df.values})\n",
    "\n",
    "def drop_outliers(df,param, quantile):\n",
    "    q = df[param].quantile(quantile)\n",
    "    return df[df[param] < q]\n",
    "\n",
    "#return new dataframe with par replaced by it's movign average\n",
    "def moving_averages(df, param, window_size):\n",
    "    _df = df.copy()\n",
    "    _df[param] = df[param].rolling(window=window_size).mean()\n",
    "    return _df\n",
    "\n",
    "def apply_features_transform(df, param, quantile=.95, depth=[100, 2500], moving_avg_window=6):\n",
    "    df = fix_date_formatting(df)\n",
    "    #df = filter_by_depth_range(df, depth[0], depth[1])\n",
    "    df = df.drop('DEPTH', axis=1)\n",
    "    df = drop_outliers(df, param, quantile)\n",
    "    df = group_by_month(df, param)\n",
    "    df = moving_averages(df, param, moving_avg_window)\n",
    "    #df = group_by_year(df, param)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b822e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584643c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders for data if they don't exist inside DATA_PATH\n",
    "for d in ([str(d) for d in DEPTHS] + ['average', 'standard']):\n",
    "    if not exists(DATA_PATH +'/'+ d):\n",
    "        makedirs(DATA_PATH + '/' + d)\n",
    "\n",
    "for file in FILENAMES:\n",
    "    ##read as csv\n",
    "    dataframe = pd.read_csv(DATA_PATH + '/' + file,index_col=False)\n",
    "    ## get parameter from column name\n",
    "    param = dataframe.columns[2]\n",
    "    ## standard\n",
    "    dataframe.to_csv(DATA_PATH + '/standard/' + file,index=False)\n",
    "    dataframe['DATE'] = pd.to_datetime(dataframe['DATE'],dayfirst = True)\n",
    "    for d in DEPTHS:\n",
    "        depth_filtered = dataframe.loc[dataframe['DEPTH'] == d]\n",
    "        #depth_filtered = group_by_month(depth_filtered, param)\n",
    "        ## save to folder for this depth\n",
    "        depth_filtered.to_csv(DATA_PATH+'/'+str(d)+'/'+file,index=False)\n",
    "    ## do the same for averaged by depth\n",
    "    average = apply_features_transform(dataframe, param)\n",
    "    average.to_csv(DATA_PATH + '/average/' + file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56435bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dataframe merging csv files in directory\n",
    "def merge(path):\n",
    "    dfs = {filename: pd.read_csv(path + '/' + filename,index_col=False )\n",
    "           for filename in FILENAMES\n",
    "           if filename.endswith('.csv') and not filename.startswith('.')}\n",
    "\n",
    "    df_list=[]\n",
    "    for df in dfs.values():\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'],dayfirst = True)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    on = ['DATE', 'DEPTH']\n",
    "    if 'DEPTH' not in df_list[0].columns:\n",
    "       on = ['DATE'] \n",
    "    df = reduce(lambda left,right: pd.merge(left.drop_duplicates(subset=on),right.drop_duplicates(subset=on),on=on,how='outer'), df_list)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform cleaning (see data_cleaning.ipynb)\n",
    "for path in ([str(i) for i in DEPTHS] + ['average', 'standard']):\n",
    "    \n",
    "    df = merge(DATA_PATH + '/' + path)\n",
    "    \n",
    "    # rest of loop is mostly copy of regression method from data_cleaning.ipynb\n",
    "    # check missing values\n",
    "    df_missing = df.drop(['DATE'],axis=1)\n",
    "    missing = df_missing.isna().sum()\n",
    "    missing = pd.DataFrame(data={'elements': missing.index,'missing':missing.values})\n",
    "    missing = missing[~missing['missing'].isin([0])]\n",
    "    missing['percentage'] =  missing['missing']/df_missing.shape[0]\n",
    "    missing.sort_values(by='percentage',ascending=False)\n",
    "\n",
    "    # check df data type\n",
    "    #df_missing[missing['elements']].info()\n",
    "    \n",
    "    # check df data type\n",
    "    df_missing[missing['elements']].info()\n",
    "    if path != 'average':\n",
    "        df_missing = df_missing.drop(['DEPTH'],axis=1)\n",
    "    df_missing = df_missing.drop(['PH'],axis=1) # PH has too many missing values\n",
    "    # df_missing.head\n",
    "    \n",
    "    X_missing = df_missing.copy()\n",
    "    y_missing = df_missing.copy()\n",
    "    y_missing.dropna(inplace=True) \n",
    "    X_missing = pd.DataFrame(X_missing)\n",
    "    y_missing = pd.DataFrame(y_missing)\n",
    "\n",
    "    # regression method\n",
    "    X_missing_reg = X_missing.copy()\n",
    "    sortindex = np.argsort(X_missing_reg.isnull().sum(axis=0)).values #sort missing columns\n",
    "\n",
    "    for i in sortindex:\n",
    "        df_ = X_missing_reg  \n",
    "        fillc = df_.iloc[:, i]  \n",
    "        df_ = pd.concat([df_.drop(df_.columns[i], axis=1), pd.DataFrame(y_missing)], axis=1)\n",
    "        df_0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0).fit_transform(df_)\n",
    "        #train and test dataset\n",
    "        Ytrain = fillc[fillc.notnull()]  # not missing part is Y_train\n",
    "        Ytest = fillc[fillc.isnull()] \n",
    "        Xtrain = df_0[Ytrain.index, :]\n",
    "        Xtest = df_0[Ytest.index, :] \n",
    "        rfc = RandomForestRegressor(n_estimators=100) \n",
    "        rfc = rfc.fit(Xtrain, Ytrain)  \n",
    "        Ypredict = rfc.predict(Xtest)\n",
    "        # put prediction values back to df\n",
    "        X_missing_reg.loc[X_missing_reg.iloc[:, i].isnull(), X_missing_reg.columns[i]] = Ypredict\n",
    "\n",
    "\n",
    "    X_missing_reg.insert(0,column = 'DATE',value=df['DATE'])\n",
    "\n",
    "    ##save csv\n",
    "    X_missing_reg.to_csv(DATA_PATH  + '/' + path + '/' + MERGED_NAME + '.csv' ,index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
